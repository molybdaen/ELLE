{
 "metadata": {
  "name": "Elastic Learning"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Introduction"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Basic Derivatives"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Softmax and Cross-Entropy Error"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We use the softmax output function to compute class probabilites $p_k$ for each example. In a supervised $K$-multi-class classification setting, the probability for an example $x$ to belong to class $C_k$ is given a-priori by $y_k = P(C_k|x)$. We measure the deviation of predicted values $p_k$ from the target values $y_k$ by means of the cross-entropy error $L(x,y)$. The derivatives of the cross-entropy error with respect to the inputs of the softmax are given below. For implementation purposes, it's much more efficient to pre-compute the derivative of $\\frac{\\partial L(softmax(x),y)}{\\partial x}$ instead of computing and multiplying the gradient $\\frac{\\partial L(p, y)}{\\partial p}$ with the Jacobean $\\frac{\\partial softmax(x)}{\\partial{x}}$"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "$$p_k(\\mathbf{x}) = softmax_k(\\mathbf{x}) = \\frac{e^{x_k}}{\\sum_i e^{x_i}}$$\nif $k = i$:\n\\begin{align}\n\\frac{\\partial p_k}{\\partial x_i} &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} + e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} - \\frac{e^{x_k}}{\\sum_i e^{x_i}} \\cdot \\frac{e^{x_i}}{\\sum_i e^{x_i}} \\\\\n &= p_k - p_k p_i\\\\\n &= p_k (1 - p_i) \n\\end{align}\n\nif $k \\ne i$:\n\n\\begin{align}\n\\frac{\\partial p_k}{\\partial x_i} &= e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n &= -p_k p_i\n\\end{align}\n$L(\\mathbf{x}, \\mathbf{y}) = - \\sum_k y_k \\cdot \\log p_k(\\mathbf{x})$\n\\begin{align}\n\\frac{\\partial L}{\\partial x_i} &= - \\sum_k y_k \\cdot \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial x_i} \\\\\n &= - y_i (1 - p_i) + \\sum_{k \\ne i} y_k p_i \\\\\n &= - y_i + \\sum_k y_k p_i \\\\\n &= - y_i + p_i \\cdot \\underbrace{\\sum_k y_k}_{= 1} \\\\\n &= p_i - y_i\n\\end{align}\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def softmax(x):\n    e = np.exp(x)\n    return e / np.sum(e)\n\ndef dsoftmax(x):\n    p = softmax(x)\n    Ds = -np.outer(p, p)\n    di = np.diag_indices(len(x))\n    Ds[di] = p-p**2.\n    return Ds\n\ndef ce_error(x, y):\n    eps = 1e-10\n    return - np.sum(y * np.log(x + eps))\n\ndef dce_error(x, y):\n    return - y * 1. / x\n\ndef dsce_error(x, y):\n    return softmax(x)-y\n\nx = np.asarray([3., 4., 2., 1.])\ny = np.asarray([0.3, 0.4, 0.2, 0.1])\n\nsoftmax_ceError_sequentially = np.dot(dce_error(softmax(x),y), dsoftmax(x))\nsoftmax_ceError_combined = dsce_error(x, y)\nprint softmax_ceError_sequentially\nprint softmax_ceError_combined",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[-0.06311718  0.24391426 -0.11285568 -0.0679414 ]\n[-0.06311718  0.24391426 -0.11285568 -0.0679414 ]\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Other NN-Derivatives: MyMatlib"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class MLib:\n    @staticmethod\n    def linear(x):\n        return x\n\n    @staticmethod\n    def dlinear(x):\n        return np.ones(x.shape)\n\n    @staticmethod\n    def tanh(x):\n        return np.tanh(x)\n\n    @staticmethod\n    def dtanh(x):\n        tan = MLib.tanh(x)\n        return (1. - tan**2.)\n\n    @staticmethod\n    def sigmoid(x):\n        return 1. / (1. + np.exp(-x))\n\n    @staticmethod\n    def dsigmoid(x):\n        sig = MLib.sigmoid(x)\n        return sig*(1.-sig)\n\n    @staticmethod\n    def softmax(x):\n        e = np.exp(x)\n        return e / np.sum(e)#[:, np.newaxis]\n\n    @staticmethod\n    def dsoftmax(x):\n        \"\"\"\n        NOTE: When computing the gradient of a combination of softmax output and crossentropy error,\n        use :func:`~slowlearning.dsce_error` instead.\n        Computing :func:`~slowlearning.dce_error` and :func:`~slowlearning.dsoftmax` separately is computationally very inefficient\n        :param x:\n        :return: Jacobean matrix. partial derivatives of all outputs :func:`~slowlearning.softmax` with respect to all inputs x\n        \"\"\"\n        p = MLib.softmax(x)\n        Ds = -np.outer(p, p)\n        di = np.diag_indices(len(x))\n        Ds[di] = p-p**2.\n        return Ds\n\n    @staticmethod\n    def rec_error(p, y):\n        return (0.5 / len(p)) * np.sum((p - y)**2.)\n\n    @staticmethod\n    def drec_error(p, y):\n        return (1. / len(p)) * (p - y)\n\n    @staticmethod\n    def dlinrec_error(x, y):\n        return x - y\n\n    @staticmethod\n    def ceb_error(p, y):\n        eps = 1e-10\n        return - np.sum(y * np.log(p + eps) + (1. - y) * np.log(1. - p + eps))\n\n    @staticmethod\n    def dceb_error(p, y):\n        return - y * 1. / p + (1. - y) / (1. - p)\n\n    @staticmethod\n    def dlogceb_error(x, y):\n        p = MLib.sigmoid(x)\n        return - y * (1. - p) + (1. - y) * p\n\n    @staticmethod\n    def cem_error(p, y):\n        eps = 1e-10\n        return - np.sum(y * np.log(p + eps))\n\n    @staticmethod\n    def dcem_error(p, y):\n        return - y * 1. / p\n\n    @staticmethod\n    def dsofcem_error(x, y):\n        return MLib.softmax(x)-y",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Utils"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "NN-Function-Wrappers"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class T_Func_Type:\n    TANH = \"tanh\"\n    LOGISTIC = \"logistic\"\n\n\nclass T_OutFunc_Type:\n    SOFTMAX = \"softmax\"\n    LINEAR = \"linear\" # linear output activation function in fact corresponds to gaussian output variables\n    LOGISTIC = \"logistic\"\n\n\nclass T_ErrFunc_Type:\n    RECONSTRUCTION = \"reconstruction\"\n    CROSS_ENTROPY_BINOMIAL = \"cross-entropy-binomial\"\n    CROSS_ENTROPY_MULTINOMIAL = \"cross-entropy-multinomial\"\n\n\nclass Function(object):\n    def __init__(self, f, df):\n        self.f = f\n        self.df = df\n\nactFuncs = {T_Func_Type.TANH: Function(MLib.tanh, MLib.dtanh),\n            T_Func_Type.LOGISTIC: Function(MLib.sigmoid, MLib.dsigmoid)}\n\noutFuncs = {T_OutFunc_Type.SOFTMAX: Function(MLib.softmax, MLib.dsoftmax),\n            T_OutFunc_Type.LINEAR: Function(MLib.linear, MLib.dlinear),\n            T_OutFunc_Type.LOGISTIC: Function(MLib.sigmoid, MLib.dsigmoid)}\n\nerrFuncs = {T_ErrFunc_Type.RECONSTRUCTION: Function(MLib.rec_error, MLib.drec_error),\n            T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL: Function(MLib.ceb_error, MLib.dceb_error),\n            T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL: Function(MLib.cem_error, MLib.dcem_error)}\n\n\nclass OutputModel(object):\n    def __init__(self, outFuncType, errFuncType):\n        self.p = outFuncs[outFuncType].f\n        self.E = errFuncs[errFuncType].f\n        if outFuncType == T_OutFunc_Type.SOFTMAX and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL:\n            self.dEdx = MLib.dsofcem_error\n        elif outFuncType == T_OutFunc_Type.LOGISTIC and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL:\n            self.dEdx = MLib.dlogceb_error\n        elif outFuncType == T_OutFunc_Type.LINEAR and errFuncType == T_ErrFunc_Type.RECONSTRUCTION:\n            self.dEdx = MLib.dlinrec_error\n        else:\n            self.dEdx = lambda x, y: errFuncs[errFuncType].df(outFuncs[outFuncType].f(x), y) * outFuncs[outFuncType].df(x)\n\n    def cost_out_grad(self, x, y):\n        out = self.p(x)\n        return (self.E(out, y), out, self.dEdx(x, y))",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Data-Utils"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "map single labeled target values to 1-of-K coding scheme"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class Utils:\n    @staticmethod\n    def code1ofK(labels, K):\n        KcodedLabels = []\n        for l in labels:\n            codedK = np.zeros(shape=(K,))\n            codedK[int(l)] = 1.\n            KcodedLabels.append(codedK)\n        return KcodedLabels",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Autoencoder"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "A basic autoencoder implementation with unsupervised pre-training and supervised fine-tuning."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class Autoencoder():\n    def __init__(self, nvis=100, nhid=50, eta=0.1, actfunc=actFuncs[T_Func_Type.TANH]):\n\n        self.visible_size = nvis\n        self.hidden_size = nhid\n\n        self.W = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), high=4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), size=(self.hidden_size, self.visible_size)), dtype=theano.config.floatX)\n        self.b1 = np.zeros(shape=(self.hidden_size,), dtype=theano.config.floatX)\n        self.b2 = np.zeros(shape=(self.visible_size,), dtype=theano.config.floatX)\n\n        self.actfunc = actfunc\n        self.eta = eta\n\n    def _encode(self, x):\n        return np.dot(self.W, x) + self.b1\n\n    def _decode(self, h):\n        return np.dot(self.W.T, h) + self.b2\n\n    def _get_rec_err(self, x, z):\n        return 0.5 * np.sum((x-z)**2.)\n\n    def _get_ce_err(self, x, z):\n        eps = 1e-10\n        return - np.sum((x * np.log(z + eps) + (1.-x) * np.log(1.-z + eps)))\n\n    def init_supervised(self, nout):\n        self.output_size = nout\n        self.Wlabel = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.output_size + self.hidden_size)), high=4 * np.sqrt(6. / (self.output_size + self.hidden_size)), size=(self.output_size, self.hidden_size)), dtype=theano.config.floatX)\n        self.blabel = np.zeros(shape=(self.output_size,), dtype=theano.config.floatX)\n        self.OutModel = OutputModel(T_OutFunc_Type.SOFTMAX, T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL)\n\n    def get_cost_grad(self, batch):\n\n        cost = 0.\n        g_W = np.zeros(self.W.shape)\n        g_b1 = np.zeros(self.b1.shape)\n        g_b2 = np.zeros(self.b2.shape)\n\n        for x in batch:\n            a = self._encode(x)\n            h = self.actfunc.f(a)\n            p = self._decode(h)\n\n            cost += MLib.rec_error(p, x)\n\n            deltaOut = MLib.drec_error(p, x)\n\n            g_W += np.outer(deltaOut, h).T\n            g_b2 += deltaOut\n\n            deltaHidden = np.dot(self.W, deltaOut) * self.actfunc.df(a)\n            g_W += np.outer(deltaHidden, x)\n            g_b1 += deltaHidden\n\n        cost /= len(batch)\n        g_W /= len(batch)\n        g_b1 /= len(batch)\n        g_b2 /= len(batch)\n\n        return cost, g_W, g_b1, g_b2\n\n    def get_supcost_grad(self, batch, targets):\n\n        batch_cost = 0.\n        g_W = np.zeros(self.W.shape)\n        g_b1 = np.zeros(self.b1.shape)\n        g_Wlabel = np.zeros(self.Wlabel.shape)\n        g_blabel = np.zeros(self.blabel.shape)\n\n        for i, x in enumerate(batch):\n            h = self._encode(x)\n            o = np.dot(self.Wlabel, h) + self.blabel\n            (cost, out, grad) = self.OutModel.cost_out_grad(o, targets[i])\n            batch_cost += cost\n\n            deltaOut = grad\n            g_Wlabel += np.outer(deltaOut, h)\n            g_blabel += deltaOut\n\n            deltaHidden = np.dot(self.Wlabel.T, deltaOut) * self.actfunc.df(h)\n            g_W += np.outer(deltaHidden, x)\n            g_b1 += deltaHidden\n\n        batch_cost /= len(batch)\n        g_W /= len(batch)\n        g_b1 /= len(batch)\n        g_Wlabel /= len(batch)\n        g_blabel /= len(batch)\n\n        return batch_cost, g_W, g_b1, g_Wlabel, g_blabel\n\n    def train(self, data, epochs=2, batch_size=20, freeIndex=0):\n        batch_num = len(data) / batch_size\n\n        for epoch in xrange(epochs):\n            total_cost = 0.\n            self.eta *= 0.99\n            for i in xrange(batch_num):\n                batch = data[i*batch_size : (i+1)*batch_size]\n                (cost, g_W, g_b1, g_b2) = self.get_cost_grad(batch)\n                total_cost += cost\n                self.W[freeIndex] -= self.eta * g_W[freeIndex]\n                self.b1[freeIndex] -= self.eta * g_b1[freeIndex]\n                self.b2 -= self.eta * g_b2\n\n            print \"Epoch: %d\" % epoch\n            print (1. / batch_num) * total_cost\n\n    def trainSupervised(self, data, targets, epochs=10, batch_size=20):\n\n        batch_num = len(data) / batch_size\n\n        for epoch in xrange(epochs):\n            total_cost = 0.\n            self.eta = 0.99\n            for batchIdx in xrange(batch_num):\n                batch = data[batchIdx * batch_size : (batchIdx+1) * batch_size]\n                batch_targets = targets[batchIdx * batch_size : (batchIdx+1) * batch_size]\n                (cost, g_W, g_b1, g_Wlabel, g_blabel) = self.get_supcost_grad(batch, batch_targets)\n                total_cost += cost\n                self.W -= self.eta * g_W\n                self.b1 -= self.eta * g_b1\n                self.Wlabel -= self.eta * g_Wlabel\n                self.blabel -= self.eta * g_blabel\n\n            print \"Epoch: %d\" % epoch\n            print (1. / batch_num) * total_cost",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}