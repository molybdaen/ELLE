{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELLE - Elastic Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics (can be ignored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "from numpy import random as rng\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as P\n",
    "import matplotlib.ticker as MT\n",
    "import matplotlib.cm as CM\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "PATH_DATA_ROOT = r\"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def dlinear(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    tan = tanh(x)\n",
    "    return (1. - tan**2.)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig*(1.-sig)\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)#[:, np.newaxis]\n",
    "\n",
    "def dsoftmax(x):\n",
    "    \"\"\"\n",
    "    NOTE: When computing the gradient of a combination of softmax output and crossentropy error,\n",
    "    use :func:`~slowlearning.dsce_error` instead.\n",
    "    Computing :func:`~slowlearning.dce_error` and :func:`~slowlearning.dsoftmax` separately is computationally very inefficient\n",
    "    :param x:\n",
    "    :return: Jacobean matrix. partial derivatives of all outputs :func:`~slowlearning.softmax` with respect to all inputs x\n",
    "    \"\"\"\n",
    "    p = softmax(x)\n",
    "    Ds = -np.outer(p, p)\n",
    "    di = np.diag_indices(len(x))\n",
    "    Ds[di] = p-p**2.\n",
    "    return Ds\n",
    "\n",
    "def rec_error(p, y):\n",
    "    return 0.5 * np.sum((p - y)**2.)\n",
    "\n",
    "def drec_error(p, y):\n",
    "    return (p - y)\n",
    "\n",
    "def dlinrec_error(x, y):\n",
    "    return x - y\n",
    "\n",
    "def ceb_error(p, y):\n",
    "    eps = 1e-10\n",
    "    return - np.sum(y * np.log(p + eps) + (1. - y) * np.log(1. - p + eps))\n",
    "\n",
    "def dceb_error(p, y):\n",
    "    return - y * 1. / p + (1. - y) / (1. - p)\n",
    "\n",
    "def dlogceb_error(x, y):\n",
    "    p = sigmoid(x)\n",
    "    return - y * (1. - p) + (1. - y) * p\n",
    "\n",
    "def cem_error(p, y):\n",
    "    eps = 1e-10\n",
    "    return - np.sum(y * np.log(p + eps))\n",
    "\n",
    "def dcem_error(p, y):\n",
    "    return - y * 1. / p\n",
    "\n",
    "def dsofcem_error(x, y):\n",
    "    return softmax(x)-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class T_Func_Type:\n",
    "    TANH = \"tanh\"\n",
    "    LOGISTIC = \"logistic\"\n",
    "\n",
    "\n",
    "class T_OutFunc_Type:\n",
    "    SOFTMAX = \"softmax\"\n",
    "    LINEAR = \"linear\" # linear output activation function in fact corresponds to gaussian output variables\n",
    "    LOGISTIC = \"logistic\"\n",
    "\n",
    "\n",
    "class T_ErrFunc_Type:\n",
    "    RECONSTRUCTION = \"reconstruction\"\n",
    "    CROSS_ENTROPY_BINOMIAL = \"cross-entropy-binomial\"\n",
    "    CROSS_ENTROPY_MULTINOMIAL = \"cross-entropy-multinomial\"\n",
    "\n",
    "\n",
    "class Function(object):\n",
    "    def __init__(self, f, df):\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "\n",
    "actFuncs = {T_Func_Type.TANH: Function(tanh, dtanh),\n",
    "            T_Func_Type.LOGISTIC: Function(sigmoid, dsigmoid)}\n",
    "\n",
    "outFuncs = {T_OutFunc_Type.SOFTMAX: Function(softmax, dsoftmax),\n",
    "            T_OutFunc_Type.LINEAR: Function(linear, dlinear),\n",
    "            T_OutFunc_Type.LOGISTIC: Function(sigmoid, dsigmoid)}\n",
    "\n",
    "errFuncs = {T_ErrFunc_Type.RECONSTRUCTION: Function(rec_error, drec_error),\n",
    "            T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL: Function(ceb_error, dceb_error),\n",
    "            T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL: Function(cem_error, dcem_error)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def code1ofK(labels, K):\n",
    "        KcodedLabels = []\n",
    "        for l in labels:\n",
    "            codedK = np.zeros(shape=(K,))\n",
    "            codedK[int(l)] = 1.\n",
    "            KcodedLabels.append(codedK)\n",
    "        return KcodedLabels\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_mnist():\n",
    "        with gzip.open(r\"../data/mnist.pkl.gz\", 'rb') as f:\n",
    "            tr,te,vl = cPickle.load(f)\n",
    "        return tr, te, vl\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(goldLabels, predictedLabels):\n",
    "        (pre, rec, f1, sup) = precision_recall_fscore_support(goldLabels, predictedLabels, beta=1.0, labels=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None)\n",
    "        np.set_printoptions(precision=3)\n",
    "        print pre\n",
    "        print rec\n",
    "        print f1\n",
    "        print sup\n",
    "        return (pre, rec, f1, sup)\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_weights(weights, panel_shape, tile_size):\n",
    "\n",
    "        def scale(x):\n",
    "            eps = 1e-8\n",
    "            x = x.copy()\n",
    "            x -= x.min()\n",
    "            x *= 1.0 / (x.max() + eps)\n",
    "            return 255.0*x\n",
    "\n",
    "        margin_y = np.zeros(tile_size[1])\n",
    "        margin_x = np.zeros((tile_size[0] + 1) * panel_shape[0])\n",
    "        image = margin_x.copy()\n",
    "\n",
    "        for y in range(panel_shape[1]):\n",
    "            tmp = np.hstack( [ np.c_[ scale( x.reshape(tile_size) ), margin_y ] for x in weights[y*panel_shape[0]:(y+1)*panel_shape[0]]])\n",
    "            tmp = np.vstack([tmp, margin_x])\n",
    "            image = np.vstack([image, tmp])\n",
    "\n",
    "        img = Image.fromarray(image)\n",
    "        img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "\n",
    "class OutputModel(object):\n",
    "    def __init__(self, outFuncType, errFuncType):\n",
    "        self.p = outFuncs[outFuncType]\n",
    "        self.E = errFuncs[errFuncType]\n",
    "        if outFuncType == T_OutFunc_Type.SOFTMAX and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL:\n",
    "            self.dEdx = dsofcem_error\n",
    "        elif outFuncType == T_OutFunc_Type.LOGISTIC and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL:\n",
    "            self.dEdx = dlogceb_error\n",
    "        elif outFuncType == T_OutFunc_Type.LINEAR and errFuncType == T_ErrFunc_Type.RECONSTRUCTION:\n",
    "            self.dEdx = dlinrec_error\n",
    "        else:\n",
    "            self.dEdx = \"composite\"\n",
    "\n",
    "    def cost_out_grad(self, x, y):\n",
    "        out = self.p.f(x)\n",
    "        if self.dEdx == \"composite\":\n",
    "            return (self.E.f(out, y), out, self.E.df(self.p.f(x), y) * self.p.df(x))\n",
    "        else:\n",
    "            return (self.E.f(out, y), out, self.dEdx(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the softmax output function to compute class probabilites $p_k$ for each example. In a supervised $K$-multi-class classification setting, the probability for an example $x$ to belong to class $C_k$ is given a-priori by $y_k = P(C_k|x)$. We measure the deviation of predicted values $p_k$ from the target values $y_k$ by means of the cross-entropy error $L(x,y)$. The derivatives of the cross-entropy error with respect to the inputs of the softmax are given below. For implementation purposes, it's much more efficient to pre-compute the derivative of $\\frac{\\partial L(softmax(x),y)}{\\partial x}$ instead of computing and multiplying the gradient $\\frac{\\partial L(p, y)}{\\partial p}$ with the Jacobean $\\frac{\\partial softmax(x)}{\\partial{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_k(\\mathbf{x}) = softmax_k(\\mathbf{x}) = \\frac{e^{x_k}}{\\sum_i e^{x_i}}$$\n",
    "if $k = i$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} + e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} - \\frac{e^{x_k}}{\\sum_i e^{x_i}} \\cdot \\frac{e^{x_i}}{\\sum_i e^{x_i}} \\\\\n",
    " &= p_k - p_k p_i\\\\\n",
    " &= p_k (1 - p_i) \n",
    "\\end{align}\n",
    "\n",
    "if $k \\ne i$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= -p_k p_i\n",
    "\\end{align}\n",
    "$L(\\mathbf{x}, \\mathbf{y}) = - \\sum_k y_k \\cdot \\log p_k(\\mathbf{x})$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial x_i} &= - \\sum_k y_k \\cdot \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial x_i} \\\\\n",
    " &= - y_i (1 - p_i) + \\sum_{k \\ne i} y_k p_i \\\\\n",
    " &= - y_i + \\sum_k y_k p_i \\\\\n",
    " &= - y_i + p_i \\cdot \\underbrace{\\sum_k y_k}_{= 1} \\\\\n",
    " &= p_i - y_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes and Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each Bit represents a linear decision boundary, partitionning the space into 2 regions. Each extra Bit again partitions each existing region into another 2 regions -> d-Bits = 2^d regions (states). Density-Estimation. In this context, figure out the relationship of: Entropy, Correlation, Frequency, Dependency of Random Variables. Buzzwords: Correlation-based learning, slow feature analysis, manifold-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, nvis=100, nhid=50, eta=0.1, actfunc=actFuncs[T_Func_Type.TANH]):\n",
    "\n",
    "        self.visible_size = nvis\n",
    "        self.hidden_size = nhid\n",
    "\n",
    "        self.W = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), high=4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), size=(self.hidden_size, self.visible_size)), dtype=theano.config.floatX)\n",
    "        self.b1 = np.zeros(shape=(self.hidden_size,), dtype=theano.config.floatX)\n",
    "        self.b2 = np.zeros(shape=(self.visible_size,), dtype=theano.config.floatX)\n",
    "\n",
    "        self.actfunc = actfunc\n",
    "        self.eta = eta\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return np.dot(self.W, x) + self.b1\n",
    "\n",
    "    def _decode(self, h):\n",
    "        return np.dot(self.W.T, h) + self.b2\n",
    "\n",
    "    def _get_rec_err(self, x, z):\n",
    "        return 0.5 * np.sum((x-z)**2.)\n",
    "\n",
    "    def _get_ce_err(self, x, z):\n",
    "        eps = 1e-10\n",
    "        return - np.sum((x * np.log(z + eps) + (1.-x) * np.log(1.-z + eps)))\n",
    "\n",
    "    def init_supervised(self, nout):\n",
    "        self.output_size = nout\n",
    "        self.Wlabel = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.output_size + self.hidden_size)), high=4 * np.sqrt(6. / (self.output_size + self.hidden_size)), size=(self.output_size, self.hidden_size)), dtype=theano.config.floatX)\n",
    "        self.blabel = np.zeros(shape=(self.output_size,), dtype=theano.config.floatX)\n",
    "        self.OutModel = OutputModel(T_OutFunc_Type.SOFTMAX, T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL)\n",
    "\n",
    "    def get_cost_grad(self, batch):\n",
    "\n",
    "        cost = 0.\n",
    "        g_W = np.zeros(self.W.shape)\n",
    "        g_b1 = np.zeros(self.b1.shape)\n",
    "        g_b2 = np.zeros(self.b2.shape)\n",
    "\n",
    "        for x in batch:\n",
    "            a = self._encode(x)\n",
    "            h = self.actfunc.f(a)\n",
    "            p = self._decode(h)\n",
    "\n",
    "            cost += rec_error(p, x)\n",
    "\n",
    "            deltaOut = drec_error(p, x)\n",
    "\n",
    "            g_W += np.outer(deltaOut, h).T\n",
    "            g_b2 += deltaOut\n",
    "\n",
    "            deltaHidden = np.dot(self.W, deltaOut) * self.actfunc.df(a)\n",
    "            g_W += np.outer(deltaHidden, x)\n",
    "            g_b1 += deltaHidden\n",
    "\n",
    "        cost /= len(batch)\n",
    "        g_W /= len(batch)\n",
    "        g_b1 /= len(batch)\n",
    "        g_b2 /= len(batch)\n",
    "\n",
    "        return cost, g_W, g_b1, g_b2\n",
    "\n",
    "    def get_supcost_grad(self, batch, targets):\n",
    "\n",
    "        batch_cost = 0.\n",
    "        g_W = np.zeros(self.W.shape)\n",
    "        g_b1 = np.zeros(self.b1.shape)\n",
    "        g_Wlabel = np.zeros(self.Wlabel.shape)\n",
    "        g_blabel = np.zeros(self.blabel.shape)\n",
    "\n",
    "        for i, x in enumerate(batch):\n",
    "            a = self._encode(x)\n",
    "            h = self.actfunc.f(a)\n",
    "            o = np.dot(self.Wlabel, h) + self.blabel\n",
    "            (cost, out, grad) = self.OutModel.cost_out_grad(o, targets[i])\n",
    "            batch_cost += cost\n",
    "\n",
    "            deltaOut = grad\n",
    "            g_Wlabel += np.outer(deltaOut, h)\n",
    "            g_blabel += deltaOut\n",
    "\n",
    "            deltaHidden = np.dot(self.Wlabel.T, deltaOut) * self.actfunc.df(a)\n",
    "            g_W += np.outer(deltaHidden, x)\n",
    "            g_b1 += deltaHidden\n",
    "\n",
    "        batch_cost /= len(batch)\n",
    "        g_W /= len(batch)\n",
    "        g_b1 /= len(batch)\n",
    "        g_Wlabel /= len(batch)\n",
    "        g_blabel /= len(batch)\n",
    "\n",
    "        return batch_cost, g_W, g_b1, g_Wlabel, g_blabel\n",
    "\n",
    "    def train(self, data, epochs=2, batch_size=20, freeIndex=0):\n",
    "\n",
    "        batch_num = len(data) / batch_size\n",
    "\n",
    "        for epoch in xrange(epochs):\n",
    "            total_cost = 0.\n",
    "            self.eta *= 0.99\n",
    "            for i in xrange(batch_num):\n",
    "                batch = data[i*batch_size : (i+1)*batch_size]\n",
    "                (cost, g_W, g_b1, g_b2) = self.get_cost_grad(batch)\n",
    "                total_cost += cost\n",
    "                self.W[freeIndex] -= self.eta * g_W[freeIndex]\n",
    "                self.b1[freeIndex] -= self.eta * g_b1[freeIndex]\n",
    "                self.b2 -= self.eta * g_b2\n",
    "\n",
    "            print \"Epoch: %d\" % epoch\n",
    "            print (1. / batch_num) * total_cost\n",
    "\n",
    "    def trainSupervised(self, data, targets, epochs=10, batch_size=20):\n",
    "\n",
    "        batch_num = len(data) / batch_size\n",
    "\n",
    "        for epoch in xrange(epochs):\n",
    "            total_cost = 0.\n",
    "            self.eta = 0.99\n",
    "            for batchIdx in xrange(batch_num):\n",
    "                batch = data[batchIdx * batch_size : (batchIdx+1) * batch_size]\n",
    "                batch_targets = targets[batchIdx * batch_size : (batchIdx+1) * batch_size]\n",
    "                (cost, g_W, g_b1, g_Wlabel, g_blabel) = self.get_supcost_grad(batch, batch_targets)\n",
    "                total_cost += cost\n",
    "                self.W -= self.eta * g_W\n",
    "                self.b1 -= self.eta * g_b1\n",
    "                self.Wlabel -= self.eta * g_Wlabel\n",
    "                self.blabel -= self.eta * g_blabel\n",
    "\n",
    "            print \"Epoch: %d\" % epoch\n",
    "            print (1. / batch_num) * total_cost\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        if self.OutModel is None:\n",
    "            return None\n",
    "\n",
    "        predictedTargets = []\n",
    "        predictedLabels = []\n",
    "\n",
    "        for x in data:\n",
    "            a = self._encode(x)\n",
    "            h = self.actfunc.f(a)\n",
    "            o = np.dot(self.Wlabel, h) + self.blabel\n",
    "            pred = self.OutModel.p.f(o)\n",
    "            idx = np.argmax(pred)\n",
    "            predictedTargets.append(pred)\n",
    "            predictedLabels.append(idx)\n",
    "\n",
    "        return (predictedTargets, predictedLabels)\n",
    "\n",
    "    def visualize_filters(self, name=None):\n",
    "        tile_size = (int(np.sqrt(self.W[0].size)), int(np.sqrt(self.W[0].size)))\n",
    "        panel_shape = (int(len(self.W)/11)+1, len(self.W) % 11)\n",
    "        img = Utils.visualize_weights(self.W, panel_shape, tile_size)\n",
    "        if name is not None:\n",
    "            img.save(name+\".png\", \"PNG\")\n",
    "        img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Elastic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElasticLearning(object):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.targets = labels\n",
    "        self.visibles = len(data[0])\n",
    "\n",
    "    def findCode(self, codeLimit=10):\n",
    "\n",
    "        self.tmpW = None\n",
    "        self.tmpb1 = None\n",
    "        self.tmpb2 = None\n",
    "\n",
    "        bitRange = range(1, codeLimit+1)\n",
    "\n",
    "        for (idx, codeSize) in enumerate(bitRange):\n",
    "            fixedRange = range(0, idx)\n",
    "            print \"CodeSize: %d/%d\" % (codeSize, codeLimit)\n",
    "            self.ae = Autoencoder(nvis=self.visibles, nhid=codeSize)\n",
    "            if codeSize == 1:\n",
    "                self.ae.train(self.data, epochs=2, freeIndex=idx)\n",
    "                self.tmpW = self.ae.W\n",
    "                self.tmpb1 = self.ae.b1\n",
    "                self.tmpb2 = self.ae.b2\n",
    "            else:\n",
    "                self.ae.W[fixedRange] = self.tmpW\n",
    "                self.ae.b1[fixedRange] = self.tmpb1\n",
    "                self.ae.b2 = self.tmpb2\n",
    "                self.ae.train(self.data, epochs=2, freeIndex=idx)\n",
    "                self.tmpW = self.ae.W\n",
    "                self.tmpb1 = self.ae.b1\n",
    "                self.tmpb2 = self.ae.b2\n",
    "\n",
    "        self.dump_model()\n",
    "\n",
    "    def finetune(self):\n",
    "        self.ae.init_supervised(len(self.targets[0]))\n",
    "        self.ae.trainSupervised(self.data, self.targets, epochs=10, batch_size=20)\n",
    "\n",
    "    def dump_model(self):\n",
    "        cPickle.dump(self.ae, open(PATH_DATA_ROOT+r\"/model.pkl\", \"w\"))\n",
    "\n",
    "    def load_model(self):\n",
    "        self.ae = cPickle.load(open(PATH_DATA_ROOT+r\"/model.pkl\", \"r\"))\n",
    "        return self.ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first experiment is a very basic one. We compare the elastic learning and the compound learning method on a single hidden layer network. \n",
    "Compound Learning:\n",
    "This is the standard backpropagation algorithm. It trains all nodes in the hidden layer simultaneously as a bunch of composed functions. As usual, the hidden layer is pre-trained via reconstruction error. After the pre-training phase (upper limit epoch=limit or $\\Delta Error \\lt \\epsilon$) we fine-tune the weights with respect to the supervised error criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Learning Algorithm:\n",
      "CodeSize: 1/10\n",
      "Epoch: 0\n",
      "26.1463134712\n",
      "Epoch: 1\n",
      "26.1119105185\n",
      "CodeSize: 2/10\n",
      "Epoch: 0\n",
      "24.5531930612\n",
      "Epoch: 1\n",
      "24.2517222973\n",
      "CodeSize: 3/10\n",
      "Epoch: 0\n",
      "24.3191618102\n",
      "Epoch: 1\n",
      "22.8561457221\n",
      "CodeSize: 4/10\n",
      "Epoch: 0\n",
      "21.7154252466\n",
      "Epoch: 1\n",
      "21.547304021\n",
      "CodeSize: 5/10\n",
      "Epoch: 0\n",
      "20.7092914072\n",
      "Epoch: 1\n",
      "20.4933635909\n",
      "CodeSize: 6/10\n",
      "Epoch: 0\n",
      "19.6232492431\n",
      "Epoch: 1\n",
      "19.5253882057\n",
      "CodeSize: 7/10\n",
      "Epoch: 0\n",
      "19.2772963155\n",
      "Epoch: 1\n",
      "18.6199839685\n",
      "CodeSize: 8/10\n",
      "Epoch: 0\n",
      "18.1329145595\n",
      "Epoch: 1\n",
      "17.8763162793\n",
      "CodeSize: 9/10\n",
      "Epoch: 0\n",
      "17.3101358058\n",
      "Epoch: 1\n",
      "17.2471438195\n",
      "CodeSize: 10/10\n",
      "Epoch: 0\n",
      "16.7070933705\n",
      "Epoch: 1\n",
      "16.6572121649\n",
      "pre-training took 67.55 s\n",
      "Fine-Tuning with label information:\n",
      "Epoch: 0\n",
      "0.84470593266\n",
      "Epoch: 1\n",
      "0.594259286759\n",
      "Epoch: 2\n",
      "0.54815990559\n",
      "Epoch: 3\n",
      "0.467092668343\n",
      "Epoch: 4\n",
      "0.444181307399\n",
      "Epoch: 5\n",
      "0.42732942827\n",
      "Epoch: 6\n",
      "0.412641888515\n",
      "Epoch: 7\n",
      "0.409795384781\n",
      "Epoch: 8\n",
      "0.40255350665\n",
      "Epoch: 9\n",
      "0.403987568678\n",
      "Test Model:\n",
      "[ 0.94   0.894  0.949  0.884  0.904  0.797  0.927  0.925  0.892  0.902]\n",
      "[ 0.941  0.955  0.867  0.849  0.922  0.878  0.946  0.939  0.828  0.879]\n",
      "[ 0.941  0.923  0.906  0.866  0.913  0.835  0.937  0.932  0.859  0.89 ]\n",
      "[ 991 1064  990 1030  983  915  967 1090 1009  961]\n",
      "Compound Learning Algorithm:\n",
      "Epoch: 0\n",
      "21.2391305757\n",
      "Epoch: 1\n",
      "18.3542937478\n",
      "Epoch: 2\n",
      "18.332309422\n",
      "Epoch: 3\n",
      "18.3240644763\n",
      "Epoch: 4\n",
      "18.3169311159\n",
      "Epoch: 5\n",
      "18.3091695212\n",
      "Epoch: 6\n",
      "18.2995402997\n",
      "Epoch: 7\n",
      "18.2921426329\n",
      "Epoch: 8\n",
      "18.2862893217\n",
      "Epoch: 9\n",
      "18.2784296486\n",
      "Epoch: 10\n",
      "18.2723720192\n",
      "Epoch: 11\n",
      "18.2662516755\n",
      "Epoch: 12\n",
      "18.2602324857\n",
      "Epoch: 13\n",
      "18.2541248462\n",
      "Epoch: 14\n",
      "18.247729345\n",
      "Epoch: 15\n",
      "18.2421277844\n",
      "Epoch: 16\n",
      "18.236484596\n",
      "Epoch: 17\n",
      "18.2308495755\n",
      "Epoch: 18\n",
      "18.2253820385\n",
      "Epoch: 19\n",
      "18.2206611116\n",
      "pre-training took 91.15 s\n",
      "Fine-Tuning with Label Information\n",
      "Epoch: 0\n",
      "0.865325835914\n",
      "Epoch: 1\n",
      "0.667060254128\n",
      "Epoch: 2\n",
      "0.591603951652\n",
      "Epoch: 3\n",
      "0.576164741969\n",
      "Epoch: 4\n",
      "0.57767234912\n",
      "Epoch: 5\n",
      "0.551896147515\n",
      "Epoch: 6\n",
      "0.546121974109\n",
      "Epoch: 7\n",
      "0.538458863137\n",
      "Epoch: 8\n",
      "0.528905982494\n",
      "Epoch: 9\n",
      "0.517738120298\n",
      "Test Model:\n",
      "[ 0.905  0.944  0.907  0.797  0.92   0.752  0.875  0.869  0.86   0.798]\n",
      "[ 0.937  0.951  0.782  0.855  0.84   0.797  0.916  0.879  0.824  0.824]\n",
      "[ 0.921  0.948  0.84   0.825  0.878  0.773  0.895  0.874  0.842  0.811]\n",
      "[ 991 1064  990 1030  983  915  967 1090 1009  961]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_data, test_data, valid_data = Utils.load_mnist()\n",
    "K = 10\n",
    "\n",
    "print \"Elastic Learning Algorithm:\"\n",
    "ella = ElasticLearning(train_data[0], Utils.code1ofK(train_data[1], 10))\n",
    "strttime = time.time()\n",
    "ella.findCode()\n",
    "endtime = time.time()\n",
    "print \"pre-training took %.2f s\" % (endtime-strttime)\n",
    "#ella.ae.visualize_filters(\"elastic-pretrained\")\n",
    "print \"Fine-Tuning with label information:\"\n",
    "ella.finetune()\n",
    "#ella.ae.visualize_filters(\"elastic-finetuned\")\n",
    "print \"Test Model:\"\n",
    "(pred_targets, pred_labels) = ella.ae.predict(test_data[0])\n",
    "(pre, rec, f1, sup) = Utils.validate(test_data[1], pred_labels)\n",
    "elastic = f1\n",
    "\n",
    "print \"Compound Learning Algorithm:\"\n",
    "ae = Autoencoder(nvis=784, nhid=10, eta=0.1)\n",
    "strttime = time.time()\n",
    "ae.train(train_data[0], epochs=20, batch_size=20, freeIndex=range(10))\n",
    "endtime = time.time()\n",
    "print \"pre-training took %.2f s\" % (endtime-strttime)\n",
    "#ae.visualize_filters(\"compound-pretrained\")\n",
    "print \"Fine-Tuning with Label Information\"\n",
    "ae.init_supervised(K)\n",
    "ae.trainSupervised(train_data[0], Utils.code1ofK(train_data[1], K), epochs=10, batch_size=20)\n",
    "#ae.visualize_filters(\"compound-finetuned\")\n",
    "print \"Test Model:\"\n",
    "(pred_targets, pred_labels) = ae.predict(test_data[0])\n",
    "(pre, rec, f1, sup) = Utils.validate(test_data[1], pred_labels)\n",
    "compound = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannesjurgovsky/anaconda/lib/python2.7/site-packages/matplotlib/tight_layout.py:225: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n"
     ]
    }
   ],
   "source": [
    "n_groups = K\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, elastic, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     error_kw=error_config,\n",
    "                     label='Elastic')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, compound, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config,\n",
    "                     label='Compound')\n",
    "\n",
    "plt.xlabel('Digit Group')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Scores on MNIST by Digit Groups and Method')\n",
    "plt.xticks(index + bar_width, ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
