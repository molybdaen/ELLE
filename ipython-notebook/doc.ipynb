{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELLE - Elastic Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics (can be ignored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLib:\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def dlinear(x):\n",
    "        return np.ones(x.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def dtanh(x):\n",
    "        tan = MLib.tanh(x)\n",
    "        return (1. - tan**2.)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def dsigmoid(x):\n",
    "        sig = MLib.sigmoid(x)\n",
    "        return sig*(1.-sig)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e)#[:, np.newaxis]\n",
    "\n",
    "    @staticmethod\n",
    "    def dsoftmax(x):\n",
    "        \"\"\"\n",
    "        NOTE: When computing the gradient of a combination of softmax output and crossentropy error,\n",
    "        use :func:`~slowlearning.dsce_error` instead.\n",
    "        Computing :func:`~slowlearning.dce_error` and :func:`~slowlearning.dsoftmax` separately is computationally very inefficient\n",
    "        :param x:\n",
    "        :return: Jacobean matrix. partial derivatives of all outputs :func:`~slowlearning.softmax` with respect to all inputs x\n",
    "        \"\"\"\n",
    "        p = MLib.softmax(x)\n",
    "        Ds = -np.outer(p, p)\n",
    "        di = np.diag_indices(len(x))\n",
    "        Ds[di] = p-p**2.\n",
    "        return Ds\n",
    "\n",
    "    @staticmethod\n",
    "    def rec_error(p, y):\n",
    "        return 0.5 * np.sum((p - y)**2.)\n",
    "\n",
    "    @staticmethod\n",
    "    def drec_error(p, y):\n",
    "        return (p - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def dlinrec_error(x, y):\n",
    "        return x - y\n",
    "\n",
    "    @staticmethod\n",
    "    def ceb_error(p, y):\n",
    "        eps = 1e-10\n",
    "        return - np.sum(y * np.log(p + eps) + (1. - y) * np.log(1. - p + eps))\n",
    "\n",
    "    @staticmethod\n",
    "    def dceb_error(p, y):\n",
    "        return - y * 1. / p + (1. - y) / (1. - p)\n",
    "\n",
    "    @staticmethod\n",
    "    def dlogceb_error(x, y):\n",
    "        p = MLib.sigmoid(x)\n",
    "        return - y * (1. - p) + (1. - y) * p\n",
    "\n",
    "    @staticmethod\n",
    "    def cem_error(p, y):\n",
    "        eps = 1e-10\n",
    "        return - np.sum(y * np.log(p + eps))\n",
    "\n",
    "    @staticmethod\n",
    "    def dcem_error(p, y):\n",
    "        return - y * 1. / p\n",
    "\n",
    "    @staticmethod\n",
    "    def dsofcem_error(x, y):\n",
    "        return MLib.softmax(x)-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class T_Func_Type:\n",
    "    TANH = \"tanh\"\n",
    "    LOGISTIC = \"logistic\"\n",
    "\n",
    "\n",
    "class T_OutFunc_Type:\n",
    "    SOFTMAX = \"softmax\"\n",
    "    LINEAR = \"linear\" # linear output activation function in fact corresponds to gaussian output variables\n",
    "    LOGISTIC = \"logistic\"\n",
    "\n",
    "\n",
    "class T_ErrFunc_Type:\n",
    "    RECONSTRUCTION = \"reconstruction\"\n",
    "    CROSS_ENTROPY_BINOMIAL = \"cross-entropy-binomial\"\n",
    "    CROSS_ENTROPY_MULTINOMIAL = \"cross-entropy-multinomial\"\n",
    "\n",
    "\n",
    "class Function(object):\n",
    "    def __init__(self, f, df):\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "\n",
    "actFuncs = {T_Func_Type.TANH: Function(MLib.tanh, MLib.dtanh),\n",
    "            T_Func_Type.LOGISTIC: Function(MLib.sigmoid, MLib.dsigmoid)}\n",
    "\n",
    "outFuncs = {T_OutFunc_Type.SOFTMAX: Function(MLib.softmax, MLib.dsoftmax),\n",
    "            T_OutFunc_Type.LINEAR: Function(MLib.linear, MLib.dlinear),\n",
    "            T_OutFunc_Type.LOGISTIC: Function(MLib.sigmoid, MLib.dsigmoid)}\n",
    "\n",
    "errFuncs = {T_ErrFunc_Type.RECONSTRUCTION: Function(MLib.rec_error, MLib.drec_error),\n",
    "            T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL: Function(MLib.ceb_error, MLib.dceb_error),\n",
    "            T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL: Function(MLib.cem_error, MLib.dcem_error)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def code1ofK(labels, K):\n",
    "        KcodedLabels = []\n",
    "        for l in labels:\n",
    "            codedK = np.zeros(shape=(K,))\n",
    "            codedK[int(l)] = 1.\n",
    "            KcodedLabels.append(codedK)\n",
    "        return KcodedLabels\n",
    "\n",
    "\n",
    "class OutputModel(object):\n",
    "    def __init__(self, outFuncType, errFuncType):\n",
    "        self.p = outFuncs[outFuncType].f\n",
    "        self.E = errFuncs[errFuncType].f\n",
    "        if outFuncType == T_OutFunc_Type.SOFTMAX and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL:\n",
    "            self.dEdx = MLib.dsofcem_error\n",
    "        elif outFuncType == T_OutFunc_Type.LOGISTIC and errFuncType == T_ErrFunc_Type.CROSS_ENTROPY_BINOMIAL:\n",
    "            self.dEdx = MLib.dlogceb_error\n",
    "        elif outFuncType == T_OutFunc_Type.LINEAR and errFuncType == T_ErrFunc_Type.RECONSTRUCTION:\n",
    "            self.dEdx = MLib.dlinrec_error\n",
    "        else:\n",
    "            self.dEdx = lambda x, y: errFuncs[errFuncType].df(outFuncs[outFuncType].f(x), y) * outFuncs[outFuncType].df(x)\n",
    "\n",
    "    def cost_out_grad(self, x, y):\n",
    "        out = self.p(x)\n",
    "        return (self.E(out, y), out, self.dEdx(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the softmax output function to compute class probabilites $p_k$ for each example. In a supervised $K$-multi-class classification setting, the probability for an example $x$ to belong to class $C_k$ is given a-priori by $y_k = P(C_k|x)$. We measure the deviation of predicted values $p_k$ from the target values $y_k$ by means of the cross-entropy error $L(x,y)$. The derivatives of the cross-entropy error with respect to the inputs of the softmax are given below. For implementation purposes, it's much more efficient to pre-compute the derivative of $\\frac{\\partial L(softmax(x),y)}{\\partial x}$ instead of computing and multiplying the gradient $\\frac{\\partial L(p, y)}{\\partial p}$ with the Jacobean $\\frac{\\partial softmax(x)}{\\partial{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_k(\\mathbf{x}) = softmax_k(\\mathbf{x}) = \\frac{e^{x_k}}{\\sum_i e^{x_i}}$$\n",
    "if $k = i$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} + e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} - \\frac{e^{x_k}}{\\sum_i e^{x_i}} \\cdot \\frac{e^{x_i}}{\\sum_i e^{x_i}} \\\\\n",
    " &= p_k - p_k p_i\\\\\n",
    " &= p_k (1 - p_i) \n",
    "\\end{align}\n",
    "\n",
    "if $k \\ne i$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= -p_k p_i\n",
    "\\end{align}\n",
    "$L(\\mathbf{x}, \\mathbf{y}) = - \\sum_k y_k \\cdot \\log p_k(\\mathbf{x})$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial x_i} &= - \\sum_k y_k \\cdot \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial x_i} \\\\\n",
    " &= - y_i (1 - p_i) + \\sum_{k \\ne i} y_k p_i \\\\\n",
    " &= - y_i + \\sum_k y_k p_i \\\\\n",
    " &= - y_i + p_i \\cdot \\underbrace{\\sum_k y_k}_{= 1} \\\\\n",
    " &= p_i - y_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes and Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each Bit represents a linear decision boundary, partitionning the space into 2 regions. Each extra Bit again partitions each existing region into another 2 regions -> d-Bits = 2^d regions (states). Density-Estimation. In this context, figure out the relationship of: Entropy, Correlation, Frequency, Dependency of Random Variables. Buzzwords: Correlation-based learning, slow feature analysis, manifold-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self, nvis=100, nhid=50, eta=0.1, actfunc=actFuncs[T_Func_Type.TANH]):\n",
    "\n",
    "        self.visible_size = nvis\n",
    "        self.hidden_size = nhid\n",
    "\n",
    "        self.W = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), high=4 * np.sqrt(6. / (self.hidden_size + self.visible_size)), size=(self.hidden_size, self.visible_size)), dtype=theano.config.floatX)\n",
    "        self.b1 = np.zeros(shape=(self.hidden_size,), dtype=theano.config.floatX)\n",
    "        self.b2 = np.zeros(shape=(self.visible_size,), dtype=theano.config.floatX)\n",
    "\n",
    "        self.actfunc = actfunc\n",
    "        self.eta = eta\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return np.dot(self.W, x) + self.b1\n",
    "\n",
    "    def _decode(self, h):\n",
    "        return np.dot(self.W.T, h) + self.b2\n",
    "\n",
    "    def _get_rec_err(self, x, z):\n",
    "        return 0.5 * np.sum((x-z)**2.)\n",
    "\n",
    "    def _get_ce_err(self, x, z):\n",
    "        eps = 1e-10\n",
    "        return - np.sum((x * np.log(z + eps) + (1.-x) * np.log(1.-z + eps)))\n",
    "\n",
    "    def init_supervised(self, nout):\n",
    "        self.output_size = nout\n",
    "        self.Wlabel = np.asarray(rng.uniform(low=-4 * np.sqrt(6. / (self.output_size + self.hidden_size)), high=4 * np.sqrt(6. / (self.output_size + self.hidden_size)), size=(self.output_size, self.hidden_size)), dtype=theano.config.floatX)\n",
    "        self.blabel = np.zeros(shape=(self.output_size,), dtype=theano.config.floatX)\n",
    "        self.OutModel = OutputModel(T_OutFunc_Type.SOFTMAX, T_ErrFunc_Type.CROSS_ENTROPY_MULTINOMIAL)\n",
    "\n",
    "    def get_cost_grad(self, batch):\n",
    "\n",
    "        cost = 0.\n",
    "        g_W = np.zeros(self.W.shape)\n",
    "        g_b1 = np.zeros(self.b1.shape)\n",
    "        g_b2 = np.zeros(self.b2.shape)\n",
    "\n",
    "        batchstel = 1. / len(batch[0])\n",
    "\n",
    "        for x in batch:\n",
    "            a = self._encode(x)\n",
    "            h = self.actfunc.f(a)\n",
    "            p = self._decode(h)\n",
    "\n",
    "            cost += batchstel * MLib.rec_error(p, x)\n",
    "\n",
    "            deltaOut = batchstel * MLib.drec_error(p, x)\n",
    "\n",
    "            g_W += np.outer(deltaOut, h).T\n",
    "            g_b2 += deltaOut\n",
    "\n",
    "            deltaHidden = np.dot(self.W, deltaOut) * self.actfunc.df(a)\n",
    "            g_W += np.outer(deltaHidden, x)\n",
    "            g_b1 += deltaHidden\n",
    "\n",
    "        cost /= len(batch)\n",
    "        g_W /= len(batch)\n",
    "        g_b1 /= len(batch)\n",
    "        g_b2 /= len(batch)\n",
    "\n",
    "        return cost, g_W, g_b1, g_b2\n",
    "\n",
    "    def get_supcost_grad(self, batch, targets):\n",
    "\n",
    "        batch_cost = 0.\n",
    "        g_W = np.zeros(self.W.shape)\n",
    "        g_b1 = np.zeros(self.b1.shape)\n",
    "        g_Wlabel = np.zeros(self.Wlabel.shape)\n",
    "        g_blabel = np.zeros(self.blabel.shape)\n",
    "\n",
    "        for i, x in enumerate(batch):\n",
    "            a = self._encode(x)\n",
    "            h = self.actfunc.f(a)\n",
    "            o = np.dot(self.Wlabel, h) + self.blabel\n",
    "            (cost, out, grad) = self.OutModel.cost_out_grad(o, targets[i])\n",
    "            batch_cost += cost\n",
    "\n",
    "            deltaOut = grad\n",
    "            g_Wlabel += np.outer(deltaOut, h)\n",
    "            g_blabel += deltaOut\n",
    "\n",
    "            deltaHidden = np.dot(self.Wlabel.T, deltaOut) * self.actfunc.df(a)\n",
    "            g_W += np.outer(deltaHidden, x)\n",
    "            g_b1 += deltaHidden\n",
    "\n",
    "        batch_cost /= len(batch)\n",
    "        g_W /= len(batch)\n",
    "        g_b1 /= len(batch)\n",
    "        g_Wlabel /= len(batch)\n",
    "        g_blabel /= len(batch)\n",
    "\n",
    "        return batch_cost, g_W, g_b1, g_Wlabel, g_blabel\n",
    "\n",
    "    def train(self, data, epochs=2, batch_size=20, freeIndex=0):\n",
    "        batch_num = len(data) / batch_size\n",
    "\n",
    "        for epoch in xrange(epochs):\n",
    "            total_cost = 0.\n",
    "            self.eta *= 0.99\n",
    "            for i in xrange(batch_num):\n",
    "                batch = data[i*batch_size : (i+1)*batch_size]\n",
    "                (cost, g_W, g_b1, g_b2) = self.get_cost_grad(batch)\n",
    "                total_cost += cost\n",
    "                self.W[freeIndex] -= self.eta * g_W[freeIndex]\n",
    "                self.b1[freeIndex] -= self.eta * g_b1[freeIndex]\n",
    "                self.b2 -= self.eta * g_b2\n",
    "\n",
    "            print \"Epoch: %d\" % epoch\n",
    "            print (1. / batch_num) * total_cost\n",
    "\n",
    "    def trainSupervised(self, data, targets, epochs=10, batch_size=20):\n",
    "\n",
    "        batch_num = len(data) / batch_size\n",
    "\n",
    "        for epoch in xrange(epochs):\n",
    "            total_cost = 0.\n",
    "            self.eta = 0.99\n",
    "            for batchIdx in xrange(batch_num):\n",
    "                batch = data[batchIdx * batch_size : (batchIdx+1) * batch_size]\n",
    "                batch_targets = targets[batchIdx * batch_size : (batchIdx+1) * batch_size]\n",
    "                (cost, g_W, g_b1, g_Wlabel, g_blabel) = self.get_supcost_grad(batch, batch_targets)\n",
    "                total_cost += cost\n",
    "                self.W -= self.eta * g_W\n",
    "                self.b1 -= self.eta * g_b1\n",
    "                self.Wlabel -= self.eta * g_Wlabel\n",
    "                self.blabel -= self.eta * g_blabel\n",
    "\n",
    "            print \"Epoch: %d\" % epoch\n",
    "            print (1. / batch_num) * total_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
