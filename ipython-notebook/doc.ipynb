{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELLE - Elastic Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics (can be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the softmax output function to compute class probabilites $p_k$ for each example. In a supervised $K$-multi-class classification setting, the probability for an example $x$ to belong to class $C_k$ is given a-priori by $y_k = P(C_k|x)$. We measure the deviation of predicted values $p_k$ from the target values $y_k$ by means of the cross-entropy error $L(x,y)$. The derivatives of the cross-entropy error with respect to the inputs of the softmax are given below. For implementation purposes, it's much more efficient to pre-compute the derivative of $\\frac{\\partial L(softmax(x),y)}{\\partial x}$ instead of computing and multiplying the gradient $\\frac{\\partial L(p, y)}{\\partial p}$ with the Jacobean $\\frac{\\partial softmax(x)}{\\partial{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_k(\\mathbf{x}) = softmax_k(\\mathbf{x}) = \\frac{e^{x_k}}{\\sum_i e^{x_i}}$$\n",
    "if $k = i$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} + e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= \\frac{e^{x_k}}{\\sum_i e^{x_i}} - \\frac{e^{x_k}}{\\sum_i e^{x_i}} \\cdot \\frac{e^{x_i}}{\\sum_i e^{x_i}} \\\\\n",
    " &= p_k - p_k p_i\\\\\n",
    " &= p_k (1 - p_i) \n",
    "\\end{align}\n",
    "\n",
    "if $k \\ne i$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial x_i} &= e^{x_k} \\cdot - \\frac{1}{(\\sum_i e^{x_i})^2} \\cdot e^{x_i} \\\\\n",
    " &= -p_k p_i\n",
    "\\end{align}\n",
    "$L(\\mathbf{x}, \\mathbf{y}) = - \\sum_k y_k \\cdot \\log p_k(\\mathbf{x})$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial x_i} &= - \\sum_k y_k \\cdot \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial x_i} \\\\\n",
    " &= - y_i (1 - p_i) + \\sum_{k \\ne i} y_k p_i \\\\\n",
    " &= - y_i + \\sum_k y_k p_i \\\\\n",
    " &= - y_i + p_i \\cdot \\underbrace{\\sum_k y_k}_{= 1} \\\\\n",
    " &= p_i - y_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([4,5,6])\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.asarray([7,10,9])\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}